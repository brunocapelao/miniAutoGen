{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import logging\n",
    "from miniautogen.pipeline.pipeline import PipelineComponent, ChatPipelineState\n",
    "from miniautogen.pipeline.components import (\n",
    "    NextAgentSelectorComponent,\n",
    "    Jinja2TemplatesComponent,\n",
    "    NextAgentMessageComponent,\n",
    "    UpdateNextAgentComponent,\n",
    "    LLMResponseComponent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from miniautogen.chat.chat import Chat\n",
    "from miniautogen.agent.agent import Agent\n",
    "from miniautogen.chat.chatadmin import ChatAdmin\n",
    "from miniautogen.pipeline.pipeline import Pipeline\n",
    "from miniautogen.pipeline.components import (\n",
    "    UserResponseComponent,\n",
    "    AgentReplyComponent,\n",
    "    TerminateChatComponent,\n",
    "    NextAgentSelectorComponent,\n",
    "    UserInputNextAgent\n",
    ")\n",
    "\n",
    "PROMPT_TEMPLATE_AGENT_SYSTEM = \"\"\"\n",
    "# Introdução\n",
    "- Você é um agente conforme descrito na seção \"SUA FUNÇÃO\".\n",
    "- Você atua em uma conversa colaborativa com uma EQUIPE DE AGENTES, focada em solucionar uma TAREFA específica.\n",
    "\n",
    "# Tarefa da Equipe\n",
    "- Objetivo da equipe: {{chat.context['goal']}}\n",
    "\n",
    "# Sua Função\n",
    "- NOME DO AGENTE: {{agent.name}}\n",
    "- DESCRIÇÃO DO AGENTE: \n",
    "{{agent.role}}\n",
    "\n",
    "# Sua Equipe de Agentes\n",
    "{% for agent in chat.agentList %}\n",
    "  - {{agent.name}}\n",
    "{% endfor %}\n",
    "\n",
    "# Dinâmica da Conversa\n",
    "- Considere TODAS as mensagens anteriores para construir sua resposta.\n",
    "- Você é o {{agent.name}}, nunca confunda sua identidade com a de outro agente.\n",
    "- Identificação do remetente: Cada mensagem terá um \"SENDER_ID\".\n",
    "\n",
    "# Instruções\n",
    "- Mantenha foco na sua função específica.\n",
    "- Contribua efetivamente para o sucesso da TAREFA.\n",
    "\n",
    "# Sua Equipe de Agentes\n",
    "- Aqui estão as descrições e especializações dos membros da equipe:\n",
    "{% for agent in chat.agentList %}\n",
    "  - {{agent.name}}\n",
    "{% endfor %}\n",
    "\n",
    "# Formato da Resposta\n",
    "- Responda apenas com o conteúdo da sua mensagem, sem incluir respostas de outros agentes.\n",
    "- Assegure que sua resposta seja relevante e contribua para o avanço da discussão.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE_USER = \"\"\"\n",
    "HISTÓRICO DE CONVERSAÇÃO:\n",
    "-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~\n",
    "[\n",
    "  {% for message in messages %}\n",
    "    {\"sender_id\": \"{{ message['sender_id'] }}\", \"message\": \"{{ message['message'] | escape }}\"}{% if not loop.last %}, {% endif %}\n",
    "  {% endfor %}\n",
    "]\n",
    "-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~\n",
    "\"\"\"\n",
    "from miniautogen.llms.llm_client import OpenAIClient\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key=os.getenv('OPENAI_API_KEY')\n",
    "openai_client = OpenAIClient(api_key=api_key)\n",
    "\n",
    "# Cria uma instância do componente com o template\n",
    "jinja_component = Jinja2TemplatesComponent()\n",
    "jinja_component.add_template(PROMPT_TEMPLATE_AGENT_SYSTEM, 'system')\n",
    "jinja_component.add_template(PROMPT_TEMPLATE_USER, 'user')\n",
    "\n",
    "UpdateNextAgent = UpdateNextAgentComponent()\n",
    "UpdateNextAgent.set_next_agent_id(\"agent_admin\")\n",
    "\n",
    "NextAgentMessage = NextAgentMessageComponent()\n",
    "NextAgentMessage.set_alternative_next(NextAgentSelectorComponent())\n",
    "\n",
    "# Configuração dos Pipelines\n",
    "pipeline_user = Pipeline([UserResponseComponent()])\n",
    "pipeline_jinja = Pipeline([jinja_component, LLMResponseComponent(openai_client)])\n",
    "pipeline_admin = Pipeline(\n",
    "    [NextAgentSelectorComponent(), AgentReplyComponent(), TerminateChatComponent()])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup do ambiente de teste\n",
    "chat_context = {'goal': 'Desenvolver um componente que salva as mensagens de markdown.'}\n",
    "chat = Chat()\n",
    "\n",
    "CONHECIMENTO_PREVIO = \"\"\"\n",
    "README DO MINIAUTOGEN:\n",
    "```README.md\n",
    "# MiniAutoGen: Biblioteca **leve e flexível** para criar agentes e conversas multi-agentes.\n",
    "\n",
    "## Sobre o MiniAutoGen\n",
    "\n",
    "O MiniAutoGen é uma biblioteca open source inovadora, projetada para capacitar aplicações de próxima geração em Modelos de Linguagem de Grande Escala (LLMs) através de conversas multi-agentes. Este framework se destaca por sua estrutura leve e flexível, ideal para desenvolvedores e pesquisadores que buscam explorar e expandir as fronteiras da IA conversacional.\n",
    "\n",
    "## Por que MiniAutoGen?\n",
    "\n",
    "### Conversas Multi-Agentes\n",
    "Capacite conversas envolvendo múltiplos agentes inteligentes, cada um com habilidades distintas, elevando a complexidade e sofisticação das interações.\n",
    "\n",
    "### Customização de Agentes\n",
    "Ajuste os agentes para atender a requisitos específicos, adaptando comportamento, reações e padrões de resposta conforme o necessário.\n",
    "\n",
    "### Flexibilidade e Modularidade\n",
    "Com o MiniAutoGen, você tem a liberdade de moldar conversações dinâmicas, permitindo iniciativas de diálogo dos agentes, reações automáticas e intervenções humanas quando necessário.\n",
    "\n",
    "### Coordenação Eficaz entre Agentes\n",
    "Utilize nosso framework para que os agentes colaborem eficientemente, visando atingir objetivos comuns em um ambiente partilhado.\n",
    "\n",
    "## Principais Componentes\n",
    "\n",
    "### Agent\n",
    "O núcleo de cada conversa, representando um agente individual com habilidades e comportamentos específicos, essencial para interações dinâmicas e autônomas.\n",
    "\n",
    "### Chat\n",
    "Gerencia sessões de chat em grupo, assegurando a manutenção eficaz do estado e contexto da conversa, essencial para a continuidade e coesão das interações.\n",
    "\n",
    "### ChatAdmin\n",
    "Um elemento-chave para a coordenação do chat em grupo, sincronizando ações e gerenciando a dinâmica da conversa para garantir uma colaboração eficiente.\n",
    "\n",
    "### Pipeline\n",
    "Automatiza e organiza as operações dos agentes, promovendo a escalabilidade e a manutenção facilitada do sistema.\n",
    "\n",
    "## Contribua com o MiniAutoGen\n",
    "\n",
    "Como um projeto open source, o MiniAutoGen convida entusiastas de IA, desenvolvedores e pesquisadores para contribuir e ajudar a moldar o futuro das conversas multi-agentes. Seu conhecimento e experiência podem ajudar a expandir as capacidades do MiniAutoGen, criando soluções mais robustas e versáteis para a comunidade de desenvolvedores.\n",
    "\n",
    "### Como Você Pode Contribuir:\n",
    "- **Desenvolvimento de Novos Recursos:** Ajude a adicionar novas funcionalidades e aprimorar as existentes.\n",
    "- **Documentação e Tutoriais:** Contribua com documentação clara e tutoriais para facilitar o uso do framework por novos usuários.\n",
    "- **Testes e Feedback:** Participe testando o framework e fornecendo feedback valioso para melhorias contínuas.\n",
    "- **Compartilhamento de Ideias e Experiências:** Partilhe suas experiências e ideias para enriquecer a comunidade e impulsionar inovações.\n",
    "\n",
    "## Comece a Contribuir Hoje\n",
    "\n",
    "Visite nosso repositório no GitHub para saber mais sobre como você pode se envolver e começar a contribuir. Junte-se a nós nessa jornada emocionante para impulsionar o avanço das conversas multi-agentes no mundo da inteligência artificial!\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "MiniAutoGen: Desenvolvendo hoje o futuro das conversas inteligentes.\n",
    "```\n",
    "\n",
    "### Arquitetura e Componentes do MiniAutoGen\n",
    "\n",
    "1. **Arquitetura Modular e Extensível:**\n",
    "   - O MiniAutoGen é projetado com uma arquitetura modular, permitindo que diferentes funções sejam encapsuladas em componentes distintos. \n",
    "   - Essa abordagem facilita a extensão e a personalização do sistema, permitindo aos desenvolvedores adicionar ou modificar componentes conforme necessário.\n",
    "\n",
    "2. **Componentes do Pipeline:**\n",
    "   - Cada componente representa uma operação ou um conjunto de operações que podem ser realizadas em uma conversa.\n",
    "   - Estes componentes são organizados em um \"pipeline\", onde o processamento de uma conversa é conduzido sequencialmente através de vários componentes.\n",
    "\n",
    "3. **Padrões de Desenvolvimento:**\n",
    "   - **Princípio da Responsabilidade Única:** Cada componente é responsável por uma tarefa específica, seguindo o princípio de responsabilidade única.\n",
    "   - **Abstração e Encapsulamento:** Os componentes são abstrações que ocultam a complexidade do processamento interno, oferecendo uma interface clara para interação com o restante do sistema.\n",
    "   - **Padrão de Projeto Decorator:** O uso de um pipeline onde componentes podem ser adicionados ou removidos dinamicamente sugere uma implementação semelhante ao padrão Decorator, permitindo a composição de comportamentos em tempo de execução.\n",
    "\n",
    "4. **Tipos de Componentes:**\n",
    "   - **UserResponseComponent:** Lida com as entradas dos usuários.\n",
    "   - **AgentReplyComponent:** Gera respostas dos agentes com base nas entradas processadas.\n",
    "   - **NextAgentSelectorComponent:** Determina qual agente deve responder em seguida, baseando-se na lógica ou estado da conversa.\n",
    "   - **TerminateChatComponent:** Avalia condições para encerrar a conversa.\n",
    "   - **OpenAIChatComponent e OpenAIThreadComponent:** Integram com a API da OpenAI para utilizar modelos de linguagem como agentes na conversa.\n",
    "\n",
    "5. **Gestão de Estado:**\n",
    "   - O estado da conversa é gerenciado e passado entre componentes. Isso permite a manutenção do contexto e a continuidade ao longo de uma sessão de chat.\n",
    "\n",
    "6. **Flexibilidade e Customização:**\n",
    "   - Os desenvolvedores podem criar componentes personalizados para atender a requisitos específicos, integrando funcionalidades externas ou lógicas de negócios complexas.\n",
    "\n",
    "### Padrões Arquitetônicos\n",
    "\n",
    "- **Arquitetura Orientada a Serviços (SOA):** Cada componente pode ser visto como um serviço, com entradas, processamento e saídas claramente definidos.\n",
    "- **Padrão Pipeline:** A sequência de processamento através de componentes distintos segue o padrão de pipeline, comum em processamento de dados e workflows.\n",
    "\n",
    "### Conclusão\n",
    "\n",
    "A arquitetura e os padrões de desenvolvimento do MiniAutoGen refletem uma abordagem moderna e modular para a construção de sistemas de conversação. A ênfase na modularidade, extensibilidade e responsabilidade única de cada componente torna o framework adaptável a uma variedade de cenários de uso, promovendo uma implementação eficiente e manutenível.\n",
    "```\n",
    "\n",
    "**Exemplo de components:**\n",
    "```\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from .pipeline import PipelineComponent\n",
    "import time\n",
    "\n",
    "class AgentReplyComponent(PipelineComponent):\n",
    "    def process(self, state):\n",
    "\n",
    "        Processa a resposta do agente atual e adiciona essa resposta ao chat em grupo.\n",
    "\n",
    "        Args:\n",
    "            state (PipelineState): Estado atual do pipeline.\n",
    "\n",
    "        Returns:\n",
    "            PipelineState: Estado atualizado do pipeline.\n",
    "\n",
    "        # Acessa o estado atual para obter informações necessárias\n",
    "        agent = state.get_state().get('selected_agent')\n",
    "        group_chat = state.get_state().get('group_chat')\n",
    "        if not agent or not group_chat:\n",
    "            raise ValueError(\"Agent e GroupChat são necessários para AgentReplyComponent.\")\n",
    "        # Implementação da geração da resposta do agente\n",
    "        try:\n",
    "            reply = agent.generate_reply(state)\n",
    "            print(reply)\n",
    "            group_chat.add_message(sender_id=agent.agent_id, message=reply)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar a resposta do agente: {e}\")\n",
    "\n",
    "        return state\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "INITIAL_MESSAGE = \"\"\"\n",
    "Refatorar este component para que fique mais abstrato e possamos utilizar diversos LLMs distintos.\n",
    "```python`\n",
    "class OpenAIComponent(PipelineComponent):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def process(self, state):\n",
    "        try:\n",
    "            prompt = state.get_state().get('prompt')\n",
    "            if not prompt:\n",
    "                raise ValueError(\n",
    "                    \"groupchat e agent são obrigatórios para OpenAIResponseComponent.\")\n",
    "            response = self._call_openai_api(prompt)\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erro em OpenAIResponseComponent: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _call_openai_api(self, prompt):\n",
    "         Realiza a chamada à API da OpenAI. \n",
    "        try:\n",
    "            return self.client.chat.completions.create(\n",
    "                model=\"gpt-4-1106-preview\",\n",
    "                messages=prompt,\n",
    "                temperature=1,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Erro ao chamar a API da OpenAI: {e}\")\n",
    "            raise\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "json_messages = [\n",
    "    {'sender_id': 'ADMIN', 'message': CONHECIMENTO_PREVIO},\n",
    "    {'sender_id': 'ADMIN', 'message': INITIAL_MESSAGE}\n",
    "]\n",
    "\n",
    "chat.add_messages(json_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_MANAGER_SYSTEM_PROMPT = \"\"\"\n",
    "Como Agente Gerente de Projeto (Product Owner), você coordena a interface entre os objetivos do projeto e a equipe de desenvolvimento. Suas principais funções são:\n",
    "\n",
    "1. **Especificação de Requisitos**: Colaborar com a equipe para definir requisitos claros e precisos, assegurando que as expectativas se alinhem com a implementação.\n",
    "2. **Validação de Código**: Examinar o código produzido para confirmar sua conformidade com os requisitos estabelecidos.\n",
    "3. **Nível de detalhes**: Fornecer informações e orientações adicionais para garantir que o código seja adequado para o propósito.\n",
    "\n",
    "VOCE NUNCA DEVE DESENVOLVER O CÓDIGO, APENAS REVISAR E VALIDAR.\n",
    "\n",
    "Instruções Operacionais:\n",
    "- Para iniciar o desenvolvimento: Use o comando \"DEV_AUTOGEN, POR FAVOR, desenvolva o código para os componentes especificados\" após completar a especificação.\n",
    "- Para concluir as revisões: Emita o comando `TERMINATE` quando o código estiver adequado.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "DEV_AUTOGEN_SYSTEM_PROMPT = \"\"\"\n",
    "**Tarefa**: Como especialista em desenvolvimento de componentes para a biblioteca MiniAutoGen, crie um componente utilizando Python, com ênfase em técnicas avançadas e melhores práticas de programação. O componente deve estar alinhado com os padrões de design da biblioteca e otimizado para interação e funcionalidade eficientes.\n",
    "\n",
    "**Habilidades e Conhecimentos Necessários**:\n",
    "1. **Python Avançado**: Use sua proficiência em Python para aplicar técnicas avançadas e boas práticas de codificação.\n",
    "2. **Programação Orientada a Objetos (POO)**: Aplique sua expertise em POO para estruturar o componente de forma eficiente e eficaz.\n",
    "3. **Arquiteturas MVC e SOA**: Incorpore conhecimentos em Model-View-Controller e Service-Oriented Architecture para garantir a organização e modularidade do componente.\n",
    "4. **Fundamentos de LLMs**: Utilize sua compreensão dos Modelos de Linguagem de Grande Escala, como GPT-3 e GPT-4, para integrar o componente com sistemas de IA conversacional.\n",
    "5. **Expertise em MiniAutoGen**: Aplique seu conhecimento específico da biblioteca MiniAutoGen para desenvolver uma solução personalizada e eficiente.\n",
    "\n",
    "**Contexto e Diretrizes**:\n",
    "1. **Integração com a Arquitetura Existente**: Seu componente deve aderir à arquitetura modular e extensível do MiniAutoGen, respeitando o princípio da responsabilidade única e os padrões de abstração e encapsulamento.\n",
    "2. **Funcionalidade Específica do Componente**: Escolha uma funcionalidade específica relevante para conversas multi-agentes. Isso pode incluir, mas não está limitado a, gestão de estado, seleção de agentes, terminação de chat, ou integração com modelos de linguagem.\n",
    "3. **Adesão aos Padrões Arquitetônicos**: Considere a SOA e o padrão de pipeline na construção do seu componente. Ele deve ter entradas, processamento e saídas bem definidos, e ser capaz de se integrar ao fluxo do pipeline existente.\n",
    "4. **Documentação e Exemplo de Código**: Forneça uma breve documentação explicando a finalidade e o funcionamento do seu componente. Inclua um exemplo de código que demonstre como ele se integra ao MiniAutoGen.\n",
    "\n",
    "**Informações Adicionais**:\n",
    "- Utilize as informações fornecidas no README do MiniAutoGen e nos detalhes arquitetônicos como referência.\n",
    "- Lembre-se de que o MiniAutoGen valoriza a flexibilidade, modularidade e a customização na criação de agentes e conversas multi-agentes.\n",
    "\n",
    "**Resultado Esperado**:\n",
    "- Um script Python contendo a implementação do seu componente.\n",
    "- Documentação associada explicando sua funcionalidade e integração no sistema MiniAutoGen.\n",
    "\n",
    "**Instruções para a Resposta**:\n",
    "1. **Código Completo**: Forneça um script Python completo que realize a tarefa.\n",
    "2. **Salvamento do Código**: Inclua a linha de comentário `# filename: <filename>.py` no início do seu código para indicar o nome do arquivo em que ele deve ser salvo.\n",
    "TODO SEU DESENVOLVIMENTO DEVE SER REALIZADO SEGUINDO A ESTRUTURA E ARQUITETURA DO MINIAUTOGEN, VEJA OS EXEMPLOS.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "agente1_data = {\n",
    "    'agent_id': 'PM_AUTOGEN',\n",
    "    'name': 'PM_AUTOGEN',\n",
    "    'role': PROJECT_MANAGER_SYSTEM_PROMPT}\n",
    "\n",
    "\n",
    "agente2_data = {\n",
    "    'agent_id': 'DEV_AUTOGEN',\n",
    "    'name': 'DEV_AUTOGEN',\n",
    "    'role': DEV_AUTOGEN_SYSTEM_PROMPT}\n",
    "\n",
    "agente3_data = {\n",
    "    'agent_id': 'ADMIN',\n",
    "    'name': 'ADMIN',\n",
    "    'role': 'ADMIN'}\n",
    "\n",
    "# Criação de Agentes\n",
    "agent1 = Agent.from_json(agente1_data)\n",
    "agent1.pipeline = pipeline_jinja  # Atribuindo o pipeline ao agente\n",
    "\n",
    "agent2 = Agent.from_json(agente2_data)\n",
    "agent2.pipeline = pipeline_jinja  # Atribuindo o pipeline_llm ao segundo agente\n",
    "\n",
    "agent3 = Agent.from_json(agente3_data)\n",
    "agent3.pipeline = pipeline_user  # Atribuindo o pipeline_user ao terceiro agente\n",
    "\n",
    "# Adicionando os agentes ao chat\n",
    "chat.add_agent(agent1)\n",
    "chat.add_agent(agent2)\n",
    "# chat.add_agent(agent3)\n",
    "\n",
    "\n",
    "# Criação e configuração do ChatAdmin\n",
    "chat_admin = ChatAdmin(\"admin\", \"Admin\", \"admin_role\",\n",
    "                       pipeline_admin, chat, \"manage_chat\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:miniautogen.chat.chatadmin:Chat Admin started.\n",
      "INFO:miniautogen.chat.chatadmin:Executing round 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': '\\n# Introdução\\n- Você é um agente conforme descrito na seção \"SUA FUNÇÃO\".\\n- Você atua em uma conversa colaborativa com uma EQUIPE DE AGENTES, focada em solucionar uma TAREFA específica.\\n\\n# Tarefa da Equipe\\n- Objetivo da equipe: \\n\\n# Sua Função\\n- NOME DO AGENTE: PM_AUTOGEN\\n- DESCRIÇÃO DO AGENTE: \\n\\nComo Agente Gerente de Projeto (Product Owner), você coordena a interface entre os objetivos do projeto e a equipe de desenvolvimento. Suas principais funções são:\\n\\n1. **Especificação de Requisitos**: Colaborar com a equipe para definir requisitos claros e precisos, assegurando que as expectativas se alinhem com a implementação.\\n2. **Validação de Código**: Examinar o código produzido para confirmar sua conformidade com os requisitos estabelecidos.\\n3. **Nível de detalhes**: Fornecer informações e orientações adicionais para garantir que o código seja adequado para o propósito.\\n\\nVOCE NUNCA DEVE DESENVOLVER O CÓDIGO, APENAS REVISAR E VALIDAR.\\n\\nInstruções Operacionais:\\n- Para iniciar o desenvolvimento: Use o comando &#34;DEV_AUTOGEN, POR FAVOR, desenvolva o código para os componentes especificados&#34; após completar a especificação.\\n- Para concluir as revisões: Emita o comando `TERMINATE` quando o código estiver adequado.\\n\\n\\n# Sua Equipe de Agentes\\n\\n  - PM_AUTOGEN\\n\\n  - DEV_AUTOGEN\\n\\n\\n# Dinâmica da Conversa\\n- Considere TODAS as mensagens anteriores para construir sua resposta.\\n- Você é o PM_AUTOGEN, nunca confunda sua identidade com a de outro agente.\\n- Identificação do remetente: Cada mensagem terá um \"SENDER_ID\".\\n\\n# Instruções\\n- Mantenha foco na sua função específica.\\n- Contribua efetivamente para o sucesso da TAREFA.\\n\\n# Sua Equipe de Agentes\\n- Aqui estão as descrições e especializações dos membros da equipe:\\n\\n  - PM_AUTOGEN\\n\\n  - DEV_AUTOGEN\\n\\n\\n# Formato da Resposta\\n- Responda apenas com o conteúdo da sua mensagem, sem incluir respostas de outros agentes.\\n- Assegure que sua resposta seja relevante e contribua para o avanço da discussão.'}, {'role': 'user', 'content': '\\nHISTÓRICO DE CONVERSAÇÃO:\\n-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~\\n[\\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nREADME DO MINIAUTOGEN:\\n```README.md\\n# MiniAutoGen: Biblioteca **leve e flexível** para criar agentes e conversas multi-agentes.\\n\\n## Sobre o MiniAutoGen\\n\\nO MiniAutoGen é uma biblioteca open source inovadora, projetada para capacitar aplicações de próxima geração em Modelos de Linguagem de Grande Escala (LLMs) através de conversas multi-agentes. Este framework se destaca por sua estrutura leve e flexível, ideal para desenvolvedores e pesquisadores que buscam explorar e expandir as fronteiras da IA conversacional.\\n\\n## Por que MiniAutoGen?\\n\\n### Conversas Multi-Agentes\\nCapacite conversas envolvendo múltiplos agentes inteligentes, cada um com habilidades distintas, elevando a complexidade e sofisticação das interações.\\n\\n### Customização de Agentes\\nAjuste os agentes para atender a requisitos específicos, adaptando comportamento, reações e padrões de resposta conforme o necessário.\\n\\n### Flexibilidade e Modularidade\\nCom o MiniAutoGen, você tem a liberdade de moldar conversações dinâmicas, permitindo iniciativas de diálogo dos agentes, reações automáticas e intervenções humanas quando necessário.\\n\\n### Coordenação Eficaz entre Agentes\\nUtilize nosso framework para que os agentes colaborem eficientemente, visando atingir objetivos comuns em um ambiente partilhado.\\n\\n## Principais Componentes\\n\\n### Agent\\nO núcleo de cada conversa, representando um agente individual com habilidades e comportamentos específicos, essencial para interações dinâmicas e autônomas.\\n\\n### Chat\\nGerencia sessões de chat em grupo, assegurando a manutenção eficaz do estado e contexto da conversa, essencial para a continuidade e coesão das interações.\\n\\n### ChatAdmin\\nUm elemento-chave para a coordenação do chat em grupo, sincronizando ações e gerenciando a dinâmica da conversa para garantir uma colaboração eficiente.\\n\\n### Pipeline\\nAutomatiza e organiza as operações dos agentes, promovendo a escalabilidade e a manutenção facilitada do sistema.\\n\\n## Contribua com o MiniAutoGen\\n\\nComo um projeto open source, o MiniAutoGen convida entusiastas de IA, desenvolvedores e pesquisadores para contribuir e ajudar a moldar o futuro das conversas multi-agentes. Seu conhecimento e experiência podem ajudar a expandir as capacidades do MiniAutoGen, criando soluções mais robustas e versáteis para a comunidade de desenvolvedores.\\n\\n### Como Você Pode Contribuir:\\n- **Desenvolvimento de Novos Recursos:** Ajude a adicionar novas funcionalidades e aprimorar as existentes.\\n- **Documentação e Tutoriais:** Contribua com documentação clara e tutoriais para facilitar o uso do framework por novos usuários.\\n- **Testes e Feedback:** Participe testando o framework e fornecendo feedback valioso para melhorias contínuas.\\n- **Compartilhamento de Ideias e Experiências:** Partilhe suas experiências e ideias para enriquecer a comunidade e impulsionar inovações.\\n\\n## Comece a Contribuir Hoje\\n\\nVisite nosso repositório no GitHub para saber mais sobre como você pode se envolver e começar a contribuir. Junte-se a nós nessa jornada emocionante para impulsionar o avanço das conversas multi-agentes no mundo da inteligência artificial!\\n\\n```\\n---\\n\\nMiniAutoGen: Desenvolvendo hoje o futuro das conversas inteligentes.\\n```\\n\\n### Arquitetura e Componentes do MiniAutoGen\\n\\n1. **Arquitetura Modular e Extensível:**\\n   - O MiniAutoGen é projetado com uma arquitetura modular, permitindo que diferentes funções sejam encapsuladas em componentes distintos. \\n   - Essa abordagem facilita a extensão e a personalização do sistema, permitindo aos desenvolvedores adicionar ou modificar componentes conforme necessário.\\n\\n2. **Componentes do Pipeline:**\\n   - Cada componente representa uma operação ou um conjunto de operações que podem ser realizadas em uma conversa.\\n   - Estes componentes são organizados em um &#34;pipeline&#34;, onde o processamento de uma conversa é conduzido sequencialmente através de vários componentes.\\n\\n3. **Padrões de Desenvolvimento:**\\n   - **Princípio da Responsabilidade Única:** Cada componente é responsável por uma tarefa específica, seguindo o princípio de responsabilidade única.\\n   - **Abstração e Encapsulamento:** Os componentes são abstrações que ocultam a complexidade do processamento interno, oferecendo uma interface clara para interação com o restante do sistema.\\n   - **Padrão de Projeto Decorator:** O uso de um pipeline onde componentes podem ser adicionados ou removidos dinamicamente sugere uma implementação semelhante ao padrão Decorator, permitindo a composição de comportamentos em tempo de execução.\\n\\n4. **Tipos de Componentes:**\\n   - **UserResponseComponent:** Lida com as entradas dos usuários.\\n   - **AgentReplyComponent:** Gera respostas dos agentes com base nas entradas processadas.\\n   - **NextAgentSelectorComponent:** Determina qual agente deve responder em seguida, baseando-se na lógica ou estado da conversa.\\n   - **TerminateChatComponent:** Avalia condições para encerrar a conversa.\\n   - **OpenAIChatComponent e OpenAIThreadComponent:** Integram com a API da OpenAI para utilizar modelos de linguagem como agentes na conversa.\\n\\n5. **Gestão de Estado:**\\n   - O estado da conversa é gerenciado e passado entre componentes. Isso permite a manutenção do contexto e a continuidade ao longo de uma sessão de chat.\\n\\n6. **Flexibilidade e Customização:**\\n   - Os desenvolvedores podem criar componentes personalizados para atender a requisitos específicos, integrando funcionalidades externas ou lógicas de negócios complexas.\\n\\n### Padrões Arquitetônicos\\n\\n- **Arquitetura Orientada a Serviços (SOA):** Cada componente pode ser visto como um serviço, com entradas, processamento e saídas claramente definidos.\\n- **Padrão Pipeline:** A sequência de processamento através de componentes distintos segue o padrão de pipeline, comum em processamento de dados e workflows.\\n\\n### Conclusão\\n\\nA arquitetura e os padrões de desenvolvimento do MiniAutoGen refletem uma abordagem moderna e modular para a construção de sistemas de conversação. A ênfase na modularidade, extensibilidade e responsabilidade única de cada componente torna o framework adaptável a uma variedade de cenários de uso, promovendo uma implementação eficiente e manutenível.\\n```\\n\\n**Exemplo de components:**\\n```\\nfrom openai import OpenAI\\nimport openai\\nimport os\\nimport logging\\nfrom dotenv import load_dotenv\\nfrom .pipeline import PipelineComponent\\nimport time\\n\\nclass AgentReplyComponent(PipelineComponent):\\n    def process(self, state):\\n\\n        Processa a resposta do agente atual e adiciona essa resposta ao chat em grupo.\\n\\n        Args:\\n            state (PipelineState): Estado atual do pipeline.\\n\\n        Returns:\\n            PipelineState: Estado atualizado do pipeline.\\n\\n        # Acessa o estado atual para obter informações necessárias\\n        agent = state.get_state().get(&#39;selected_agent&#39;)\\n        group_chat = state.get_state().get(&#39;group_chat&#39;)\\n        if not agent or not group_chat:\\n            raise ValueError(&#34;Agent e GroupChat são necessários para AgentReplyComponent.&#34;)\\n        # Implementação da geração da resposta do agente\\n        try:\\n            reply = agent.generate_reply(state)\\n            print(reply)\\n            group_chat.add_message(sender_id=agent.agent_id, message=reply)\\n        except Exception as e:\\n            print(f&#34;Erro ao processar a resposta do agente: {e}&#34;)\\n\\n        return state\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nRefatorar este component para que fique mais abstrato e possamos utilizar diversos LLMs distintos.\\n```python`\\nclass OpenAIComponent(PipelineComponent):\\n\\n    def __init__(self):\\n        self.client = OpenAI(api_key=os.getenv(&#39;OPENAI_API_KEY&#39;))\\n        self.logger = logging.getLogger(__name__)\\n\\n    def process(self, state):\\n        try:\\n            prompt = state.get_state().get(&#39;prompt&#39;)\\n            if not prompt:\\n                raise ValueError(\\n                    &#34;groupchat e agent são obrigatórios para OpenAIResponseComponent.&#34;)\\n            response = self._call_openai_api(prompt)\\n            return response.choices[0].message.content\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro em OpenAIResponseComponent: {e}&#34;)\\n            raise\\n\\n    def _call_openai_api(self, prompt):\\n         Realiza a chamada à API da OpenAI. \\n        try:\\n            return self.client.chat.completions.create(\\n                model=&#34;gpt-4-1106-preview&#34;,\\n                messages=prompt,\\n                temperature=1,\\n            )\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro ao chamar a API da OpenAI: {e}&#34;)\\n            raise\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nREADME DO MINIAUTOGEN:\\n```README.md\\n# MiniAutoGen: Biblioteca **leve e flexível** para criar agentes e conversas multi-agentes.\\n\\n## Sobre o MiniAutoGen\\n\\nO MiniAutoGen é uma biblioteca open source inovadora, projetada para capacitar aplicações de próxima geração em Modelos de Linguagem de Grande Escala (LLMs) através de conversas multi-agentes. Este framework se destaca por sua estrutura leve e flexível, ideal para desenvolvedores e pesquisadores que buscam explorar e expandir as fronteiras da IA conversacional.\\n\\n## Por que MiniAutoGen?\\n\\n### Conversas Multi-Agentes\\nCapacite conversas envolvendo múltiplos agentes inteligentes, cada um com habilidades distintas, elevando a complexidade e sofisticação das interações.\\n\\n### Customização de Agentes\\nAjuste os agentes para atender a requisitos específicos, adaptando comportamento, reações e padrões de resposta conforme o necessário.\\n\\n### Flexibilidade e Modularidade\\nCom o MiniAutoGen, você tem a liberdade de moldar conversações dinâmicas, permitindo iniciativas de diálogo dos agentes, reações automáticas e intervenções humanas quando necessário.\\n\\n### Coordenação Eficaz entre Agentes\\nUtilize nosso framework para que os agentes colaborem eficientemente, visando atingir objetivos comuns em um ambiente partilhado.\\n\\n## Principais Componentes\\n\\n### Agent\\nO núcleo de cada conversa, representando um agente individual com habilidades e comportamentos específicos, essencial para interações dinâmicas e autônomas.\\n\\n### Chat\\nGerencia sessões de chat em grupo, assegurando a manutenção eficaz do estado e contexto da conversa, essencial para a continuidade e coesão das interações.\\n\\n### ChatAdmin\\nUm elemento-chave para a coordenação do chat em grupo, sincronizando ações e gerenciando a dinâmica da conversa para garantir uma colaboração eficiente.\\n\\n### Pipeline\\nAutomatiza e organiza as operações dos agentes, promovendo a escalabilidade e a manutenção facilitada do sistema.\\n\\n## Contribua com o MiniAutoGen\\n\\nComo um projeto open source, o MiniAutoGen convida entusiastas de IA, desenvolvedores e pesquisadores para contribuir e ajudar a moldar o futuro das conversas multi-agentes. Seu conhecimento e experiência podem ajudar a expandir as capacidades do MiniAutoGen, criando soluções mais robustas e versáteis para a comunidade de desenvolvedores.\\n\\n### Como Você Pode Contribuir:\\n- **Desenvolvimento de Novos Recursos:** Ajude a adicionar novas funcionalidades e aprimorar as existentes.\\n- **Documentação e Tutoriais:** Contribua com documentação clara e tutoriais para facilitar o uso do framework por novos usuários.\\n- **Testes e Feedback:** Participe testando o framework e fornecendo feedback valioso para melhorias contínuas.\\n- **Compartilhamento de Ideias e Experiências:** Partilhe suas experiências e ideias para enriquecer a comunidade e impulsionar inovações.\\n\\n## Comece a Contribuir Hoje\\n\\nVisite nosso repositório no GitHub para saber mais sobre como você pode se envolver e começar a contribuir. Junte-se a nós nessa jornada emocionante para impulsionar o avanço das conversas multi-agentes no mundo da inteligência artificial!\\n\\n```\\n---\\n\\nMiniAutoGen: Desenvolvendo hoje o futuro das conversas inteligentes.\\n```\\n\\n### Arquitetura e Componentes do MiniAutoGen\\n\\n1. **Arquitetura Modular e Extensível:**\\n   - O MiniAutoGen é projetado com uma arquitetura modular, permitindo que diferentes funções sejam encapsuladas em componentes distintos. \\n   - Essa abordagem facilita a extensão e a personalização do sistema, permitindo aos desenvolvedores adicionar ou modificar componentes conforme necessário.\\n\\n2. **Componentes do Pipeline:**\\n   - Cada componente representa uma operação ou um conjunto de operações que podem ser realizadas em uma conversa.\\n   - Estes componentes são organizados em um &#34;pipeline&#34;, onde o processamento de uma conversa é conduzido sequencialmente através de vários componentes.\\n\\n3. **Padrões de Desenvolvimento:**\\n   - **Princípio da Responsabilidade Única:** Cada componente é responsável por uma tarefa específica, seguindo o princípio de responsabilidade única.\\n   - **Abstração e Encapsulamento:** Os componentes são abstrações que ocultam a complexidade do processamento interno, oferecendo uma interface clara para interação com o restante do sistema.\\n   - **Padrão de Projeto Decorator:** O uso de um pipeline onde componentes podem ser adicionados ou removidos dinamicamente sugere uma implementação semelhante ao padrão Decorator, permitindo a composição de comportamentos em tempo de execução.\\n\\n4. **Tipos de Componentes:**\\n   - **UserResponseComponent:** Lida com as entradas dos usuários.\\n   - **AgentReplyComponent:** Gera respostas dos agentes com base nas entradas processadas.\\n   - **NextAgentSelectorComponent:** Determina qual agente deve responder em seguida, baseando-se na lógica ou estado da conversa.\\n   - **TerminateChatComponent:** Avalia condições para encerrar a conversa.\\n   - **OpenAIChatComponent e OpenAIThreadComponent:** Integram com a API da OpenAI para utilizar modelos de linguagem como agentes na conversa.\\n\\n5. **Gestão de Estado:**\\n   - O estado da conversa é gerenciado e passado entre componentes. Isso permite a manutenção do contexto e a continuidade ao longo de uma sessão de chat.\\n\\n6. **Flexibilidade e Customização:**\\n   - Os desenvolvedores podem criar componentes personalizados para atender a requisitos específicos, integrando funcionalidades externas ou lógicas de negócios complexas.\\n\\n### Padrões Arquitetônicos\\n\\n- **Arquitetura Orientada a Serviços (SOA):** Cada componente pode ser visto como um serviço, com entradas, processamento e saídas claramente definidos.\\n- **Padrão Pipeline:** A sequência de processamento através de componentes distintos segue o padrão de pipeline, comum em processamento de dados e workflows.\\n\\n### Conclusão\\n\\nA arquitetura e os padrões de desenvolvimento do MiniAutoGen refletem uma abordagem moderna e modular para a construção de sistemas de conversação. A ênfase na modularidade, extensibilidade e responsabilidade única de cada componente torna o framework adaptável a uma variedade de cenários de uso, promovendo uma implementação eficiente e manutenível.\\n```\\n\\n**Exemplo de components:**\\n```\\nfrom openai import OpenAI\\nimport openai\\nimport os\\nimport logging\\nfrom dotenv import load_dotenv\\nfrom .pipeline import PipelineComponent\\nimport time\\n\\nclass AgentReplyComponent(PipelineComponent):\\n    def process(self, state):\\n\\n        Processa a resposta do agente atual e adiciona essa resposta ao chat em grupo.\\n\\n        Args:\\n            state (PipelineState): Estado atual do pipeline.\\n\\n        Returns:\\n            PipelineState: Estado atualizado do pipeline.\\n\\n        # Acessa o estado atual para obter informações necessárias\\n        agent = state.get_state().get(&#39;selected_agent&#39;)\\n        group_chat = state.get_state().get(&#39;group_chat&#39;)\\n        if not agent or not group_chat:\\n            raise ValueError(&#34;Agent e GroupChat são necessários para AgentReplyComponent.&#34;)\\n        # Implementação da geração da resposta do agente\\n        try:\\n            reply = agent.generate_reply(state)\\n            print(reply)\\n            group_chat.add_message(sender_id=agent.agent_id, message=reply)\\n        except Exception as e:\\n            print(f&#34;Erro ao processar a resposta do agente: {e}&#34;)\\n\\n        return state\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nRefatorar este component para que fique mais abstrato e possamos utilizar diversos LLMs distintos.\\n```python`\\nclass OpenAIComponent(PipelineComponent):\\n\\n    def __init__(self):\\n        self.client = OpenAI(api_key=os.getenv(&#39;OPENAI_API_KEY&#39;))\\n        self.logger = logging.getLogger(__name__)\\n\\n    def process(self, state):\\n        try:\\n            prompt = state.get_state().get(&#39;prompt&#39;)\\n            if not prompt:\\n                raise ValueError(\\n                    &#34;groupchat e agent são obrigatórios para OpenAIResponseComponent.&#34;)\\n            response = self._call_openai_api(prompt)\\n            return response.choices[0].message.content\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro em OpenAIResponseComponent: {e}&#34;)\\n            raise\\n\\n    def _call_openai_api(self, prompt):\\n         Realiza a chamada à API da OpenAI. \\n        try:\\n            return self.client.chat.completions.create(\\n                model=&#34;gpt-4-1106-preview&#34;,\\n                messages=prompt,\\n                temperature=1,\\n            )\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro ao chamar a API da OpenAI: {e}&#34;)\\n            raise\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nREADME DO MINIAUTOGEN:\\n```README.md\\n# MiniAutoGen: Biblioteca **leve e flexível** para criar agentes e conversas multi-agentes.\\n\\n## Sobre o MiniAutoGen\\n\\nO MiniAutoGen é uma biblioteca open source inovadora, projetada para capacitar aplicações de próxima geração em Modelos de Linguagem de Grande Escala (LLMs) através de conversas multi-agentes. Este framework se destaca por sua estrutura leve e flexível, ideal para desenvolvedores e pesquisadores que buscam explorar e expandir as fronteiras da IA conversacional.\\n\\n## Por que MiniAutoGen?\\n\\n### Conversas Multi-Agentes\\nCapacite conversas envolvendo múltiplos agentes inteligentes, cada um com habilidades distintas, elevando a complexidade e sofisticação das interações.\\n\\n### Customização de Agentes\\nAjuste os agentes para atender a requisitos específicos, adaptando comportamento, reações e padrões de resposta conforme o necessário.\\n\\n### Flexibilidade e Modularidade\\nCom o MiniAutoGen, você tem a liberdade de moldar conversações dinâmicas, permitindo iniciativas de diálogo dos agentes, reações automáticas e intervenções humanas quando necessário.\\n\\n### Coordenação Eficaz entre Agentes\\nUtilize nosso framework para que os agentes colaborem eficientemente, visando atingir objetivos comuns em um ambiente partilhado.\\n\\n## Principais Componentes\\n\\n### Agent\\nO núcleo de cada conversa, representando um agente individual com habilidades e comportamentos específicos, essencial para interações dinâmicas e autônomas.\\n\\n### Chat\\nGerencia sessões de chat em grupo, assegurando a manutenção eficaz do estado e contexto da conversa, essencial para a continuidade e coesão das interações.\\n\\n### ChatAdmin\\nUm elemento-chave para a coordenação do chat em grupo, sincronizando ações e gerenciando a dinâmica da conversa para garantir uma colaboração eficiente.\\n\\n### Pipeline\\nAutomatiza e organiza as operações dos agentes, promovendo a escalabilidade e a manutenção facilitada do sistema.\\n\\n## Contribua com o MiniAutoGen\\n\\nComo um projeto open source, o MiniAutoGen convida entusiastas de IA, desenvolvedores e pesquisadores para contribuir e ajudar a moldar o futuro das conversas multi-agentes. Seu conhecimento e experiência podem ajudar a expandir as capacidades do MiniAutoGen, criando soluções mais robustas e versáteis para a comunidade de desenvolvedores.\\n\\n### Como Você Pode Contribuir:\\n- **Desenvolvimento de Novos Recursos:** Ajude a adicionar novas funcionalidades e aprimorar as existentes.\\n- **Documentação e Tutoriais:** Contribua com documentação clara e tutoriais para facilitar o uso do framework por novos usuários.\\n- **Testes e Feedback:** Participe testando o framework e fornecendo feedback valioso para melhorias contínuas.\\n- **Compartilhamento de Ideias e Experiências:** Partilhe suas experiências e ideias para enriquecer a comunidade e impulsionar inovações.\\n\\n## Comece a Contribuir Hoje\\n\\nVisite nosso repositório no GitHub para saber mais sobre como você pode se envolver e começar a contribuir. Junte-se a nós nessa jornada emocionante para impulsionar o avanço das conversas multi-agentes no mundo da inteligência artificial!\\n\\n```\\n---\\n\\nMiniAutoGen: Desenvolvendo hoje o futuro das conversas inteligentes.\\n```\\n\\n### Arquitetura e Componentes do MiniAutoGen\\n\\n1. **Arquitetura Modular e Extensível:**\\n   - O MiniAutoGen é projetado com uma arquitetura modular, permitindo que diferentes funções sejam encapsuladas em componentes distintos. \\n   - Essa abordagem facilita a extensão e a personalização do sistema, permitindo aos desenvolvedores adicionar ou modificar componentes conforme necessário.\\n\\n2. **Componentes do Pipeline:**\\n   - Cada componente representa uma operação ou um conjunto de operações que podem ser realizadas em uma conversa.\\n   - Estes componentes são organizados em um &#34;pipeline&#34;, onde o processamento de uma conversa é conduzido sequencialmente através de vários componentes.\\n\\n3. **Padrões de Desenvolvimento:**\\n   - **Princípio da Responsabilidade Única:** Cada componente é responsável por uma tarefa específica, seguindo o princípio de responsabilidade única.\\n   - **Abstração e Encapsulamento:** Os componentes são abstrações que ocultam a complexidade do processamento interno, oferecendo uma interface clara para interação com o restante do sistema.\\n   - **Padrão de Projeto Decorator:** O uso de um pipeline onde componentes podem ser adicionados ou removidos dinamicamente sugere uma implementação semelhante ao padrão Decorator, permitindo a composição de comportamentos em tempo de execução.\\n\\n4. **Tipos de Componentes:**\\n   - **UserResponseComponent:** Lida com as entradas dos usuários.\\n   - **AgentReplyComponent:** Gera respostas dos agentes com base nas entradas processadas.\\n   - **NextAgentSelectorComponent:** Determina qual agente deve responder em seguida, baseando-se na lógica ou estado da conversa.\\n   - **TerminateChatComponent:** Avalia condições para encerrar a conversa.\\n   - **OpenAIChatComponent e OpenAIThreadComponent:** Integram com a API da OpenAI para utilizar modelos de linguagem como agentes na conversa.\\n\\n5. **Gestão de Estado:**\\n   - O estado da conversa é gerenciado e passado entre componentes. Isso permite a manutenção do contexto e a continuidade ao longo de uma sessão de chat.\\n\\n6. **Flexibilidade e Customização:**\\n   - Os desenvolvedores podem criar componentes personalizados para atender a requisitos específicos, integrando funcionalidades externas ou lógicas de negócios complexas.\\n\\n### Padrões Arquitetônicos\\n\\n- **Arquitetura Orientada a Serviços (SOA):** Cada componente pode ser visto como um serviço, com entradas, processamento e saídas claramente definidos.\\n- **Padrão Pipeline:** A sequência de processamento através de componentes distintos segue o padrão de pipeline, comum em processamento de dados e workflows.\\n\\n### Conclusão\\n\\nA arquitetura e os padrões de desenvolvimento do MiniAutoGen refletem uma abordagem moderna e modular para a construção de sistemas de conversação. A ênfase na modularidade, extensibilidade e responsabilidade única de cada componente torna o framework adaptável a uma variedade de cenários de uso, promovendo uma implementação eficiente e manutenível.\\n```\\n\\n**Exemplo de components:**\\n```\\nfrom openai import OpenAI\\nimport openai\\nimport os\\nimport logging\\nfrom dotenv import load_dotenv\\nfrom .pipeline import PipelineComponent\\nimport time\\n\\nclass AgentReplyComponent(PipelineComponent):\\n    def process(self, state):\\n\\n        Processa a resposta do agente atual e adiciona essa resposta ao chat em grupo.\\n\\n        Args:\\n            state (PipelineState): Estado atual do pipeline.\\n\\n        Returns:\\n            PipelineState: Estado atualizado do pipeline.\\n\\n        # Acessa o estado atual para obter informações necessárias\\n        agent = state.get_state().get(&#39;selected_agent&#39;)\\n        group_chat = state.get_state().get(&#39;group_chat&#39;)\\n        if not agent or not group_chat:\\n            raise ValueError(&#34;Agent e GroupChat são necessários para AgentReplyComponent.&#34;)\\n        # Implementação da geração da resposta do agente\\n        try:\\n            reply = agent.generate_reply(state)\\n            print(reply)\\n            group_chat.add_message(sender_id=agent.agent_id, message=reply)\\n        except Exception as e:\\n            print(f&#34;Erro ao processar a resposta do agente: {e}&#34;)\\n\\n        return state\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nRefatorar este component para que fique mais abstrato e possamos utilizar diversos LLMs distintos.\\n```python`\\nclass OpenAIComponent(PipelineComponent):\\n\\n    def __init__(self):\\n        self.client = OpenAI(api_key=os.getenv(&#39;OPENAI_API_KEY&#39;))\\n        self.logger = logging.getLogger(__name__)\\n\\n    def process(self, state):\\n        try:\\n            prompt = state.get_state().get(&#39;prompt&#39;)\\n            if not prompt:\\n                raise ValueError(\\n                    &#34;groupchat e agent são obrigatórios para OpenAIResponseComponent.&#34;)\\n            response = self._call_openai_api(prompt)\\n            return response.choices[0].message.content\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro em OpenAIResponseComponent: {e}&#34;)\\n            raise\\n\\n    def _call_openai_api(self, prompt):\\n         Realiza a chamada à API da OpenAI. \\n        try:\\n            return self.client.chat.completions.create(\\n                model=&#34;gpt-4-1106-preview&#34;,\\n                messages=prompt,\\n                temperature=1,\\n            )\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro ao chamar a API da OpenAI: {e}&#34;)\\n            raise\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nREADME DO MINIAUTOGEN:\\n```README.md\\n# MiniAutoGen: Biblioteca **leve e flexível** para criar agentes e conversas multi-agentes.\\n\\n## Sobre o MiniAutoGen\\n\\nO MiniAutoGen é uma biblioteca open source inovadora, projetada para capacitar aplicações de próxima geração em Modelos de Linguagem de Grande Escala (LLMs) através de conversas multi-agentes. Este framework se destaca por sua estrutura leve e flexível, ideal para desenvolvedores e pesquisadores que buscam explorar e expandir as fronteiras da IA conversacional.\\n\\n## Por que MiniAutoGen?\\n\\n### Conversas Multi-Agentes\\nCapacite conversas envolvendo múltiplos agentes inteligentes, cada um com habilidades distintas, elevando a complexidade e sofisticação das interações.\\n\\n### Customização de Agentes\\nAjuste os agentes para atender a requisitos específicos, adaptando comportamento, reações e padrões de resposta conforme o necessário.\\n\\n### Flexibilidade e Modularidade\\nCom o MiniAutoGen, você tem a liberdade de moldar conversações dinâmicas, permitindo iniciativas de diálogo dos agentes, reações automáticas e intervenções humanas quando necessário.\\n\\n### Coordenação Eficaz entre Agentes\\nUtilize nosso framework para que os agentes colaborem eficientemente, visando atingir objetivos comuns em um ambiente partilhado.\\n\\n## Principais Componentes\\n\\n### Agent\\nO núcleo de cada conversa, representando um agente individual com habilidades e comportamentos específicos, essencial para interações dinâmicas e autônomas.\\n\\n### Chat\\nGerencia sessões de chat em grupo, assegurando a manutenção eficaz do estado e contexto da conversa, essencial para a continuidade e coesão das interações.\\n\\n### ChatAdmin\\nUm elemento-chave para a coordenação do chat em grupo, sincronizando ações e gerenciando a dinâmica da conversa para garantir uma colaboração eficiente.\\n\\n### Pipeline\\nAutomatiza e organiza as operações dos agentes, promovendo a escalabilidade e a manutenção facilitada do sistema.\\n\\n## Contribua com o MiniAutoGen\\n\\nComo um projeto open source, o MiniAutoGen convida entusiastas de IA, desenvolvedores e pesquisadores para contribuir e ajudar a moldar o futuro das conversas multi-agentes. Seu conhecimento e experiência podem ajudar a expandir as capacidades do MiniAutoGen, criando soluções mais robustas e versáteis para a comunidade de desenvolvedores.\\n\\n### Como Você Pode Contribuir:\\n- **Desenvolvimento de Novos Recursos:** Ajude a adicionar novas funcionalidades e aprimorar as existentes.\\n- **Documentação e Tutoriais:** Contribua com documentação clara e tutoriais para facilitar o uso do framework por novos usuários.\\n- **Testes e Feedback:** Participe testando o framework e fornecendo feedback valioso para melhorias contínuas.\\n- **Compartilhamento de Ideias e Experiências:** Partilhe suas experiências e ideias para enriquecer a comunidade e impulsionar inovações.\\n\\n## Comece a Contribuir Hoje\\n\\nVisite nosso repositório no GitHub para saber mais sobre como você pode se envolver e começar a contribuir. Junte-se a nós nessa jornada emocionante para impulsionar o avanço das conversas multi-agentes no mundo da inteligência artificial!\\n\\n```\\n---\\n\\nMiniAutoGen: Desenvolvendo hoje o futuro das conversas inteligentes.\\n```\\n\\n### Arquitetura e Componentes do MiniAutoGen\\n\\n1. **Arquitetura Modular e Extensível:**\\n   - O MiniAutoGen é projetado com uma arquitetura modular, permitindo que diferentes funções sejam encapsuladas em componentes distintos. \\n   - Essa abordagem facilita a extensão e a personalização do sistema, permitindo aos desenvolvedores adicionar ou modificar componentes conforme necessário.\\n\\n2. **Componentes do Pipeline:**\\n   - Cada componente representa uma operação ou um conjunto de operações que podem ser realizadas em uma conversa.\\n   - Estes componentes são organizados em um &#34;pipeline&#34;, onde o processamento de uma conversa é conduzido sequencialmente através de vários componentes.\\n\\n3. **Padrões de Desenvolvimento:**\\n   - **Princípio da Responsabilidade Única:** Cada componente é responsável por uma tarefa específica, seguindo o princípio de responsabilidade única.\\n   - **Abstração e Encapsulamento:** Os componentes são abstrações que ocultam a complexidade do processamento interno, oferecendo uma interface clara para interação com o restante do sistema.\\n   - **Padrão de Projeto Decorator:** O uso de um pipeline onde componentes podem ser adicionados ou removidos dinamicamente sugere uma implementação semelhante ao padrão Decorator, permitindo a composição de comportamentos em tempo de execução.\\n\\n4. **Tipos de Componentes:**\\n   - **UserResponseComponent:** Lida com as entradas dos usuários.\\n   - **AgentReplyComponent:** Gera respostas dos agentes com base nas entradas processadas.\\n   - **NextAgentSelectorComponent:** Determina qual agente deve responder em seguida, baseando-se na lógica ou estado da conversa.\\n   - **TerminateChatComponent:** Avalia condições para encerrar a conversa.\\n   - **OpenAIChatComponent e OpenAIThreadComponent:** Integram com a API da OpenAI para utilizar modelos de linguagem como agentes na conversa.\\n\\n5. **Gestão de Estado:**\\n   - O estado da conversa é gerenciado e passado entre componentes. Isso permite a manutenção do contexto e a continuidade ao longo de uma sessão de chat.\\n\\n6. **Flexibilidade e Customização:**\\n   - Os desenvolvedores podem criar componentes personalizados para atender a requisitos específicos, integrando funcionalidades externas ou lógicas de negócios complexas.\\n\\n### Padrões Arquitetônicos\\n\\n- **Arquitetura Orientada a Serviços (SOA):** Cada componente pode ser visto como um serviço, com entradas, processamento e saídas claramente definidos.\\n- **Padrão Pipeline:** A sequência de processamento através de componentes distintos segue o padrão de pipeline, comum em processamento de dados e workflows.\\n\\n### Conclusão\\n\\nA arquitetura e os padrões de desenvolvimento do MiniAutoGen refletem uma abordagem moderna e modular para a construção de sistemas de conversação. A ênfase na modularidade, extensibilidade e responsabilidade única de cada componente torna o framework adaptável a uma variedade de cenários de uso, promovendo uma implementação eficiente e manutenível.\\n```\\n\\n**Exemplo de components:**\\n```\\nfrom openai import OpenAI\\nimport openai\\nimport os\\nimport logging\\nfrom dotenv import load_dotenv\\nfrom .pipeline import PipelineComponent\\nimport time\\n\\nclass AgentReplyComponent(PipelineComponent):\\n    def process(self, state):\\n\\n        Processa a resposta do agente atual e adiciona essa resposta ao chat em grupo.\\n\\n        Args:\\n            state (PipelineState): Estado atual do pipeline.\\n\\n        Returns:\\n            PipelineState: Estado atualizado do pipeline.\\n\\n        # Acessa o estado atual para obter informações necessárias\\n        agent = state.get_state().get(&#39;selected_agent&#39;)\\n        group_chat = state.get_state().get(&#39;group_chat&#39;)\\n        if not agent or not group_chat:\\n            raise ValueError(&#34;Agent e GroupChat são necessários para AgentReplyComponent.&#34;)\\n        # Implementação da geração da resposta do agente\\n        try:\\n            reply = agent.generate_reply(state)\\n            print(reply)\\n            group_chat.add_message(sender_id=agent.agent_id, message=reply)\\n        except Exception as e:\\n            print(f&#34;Erro ao processar a resposta do agente: {e}&#34;)\\n\\n        return state\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nRefatorar este component para que fique mais abstrato e possamos utilizar diversos LLMs distintos.\\n```python`\\nclass OpenAIComponent(PipelineComponent):\\n\\n    def __init__(self):\\n        self.client = OpenAI(api_key=os.getenv(&#39;OPENAI_API_KEY&#39;))\\n        self.logger = logging.getLogger(__name__)\\n\\n    def process(self, state):\\n        try:\\n            prompt = state.get_state().get(&#39;prompt&#39;)\\n            if not prompt:\\n                raise ValueError(\\n                    &#34;groupchat e agent são obrigatórios para OpenAIResponseComponent.&#34;)\\n            response = self._call_openai_api(prompt)\\n            return response.choices[0].message.content\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro em OpenAIResponseComponent: {e}&#34;)\\n            raise\\n\\n    def _call_openai_api(self, prompt):\\n         Realiza a chamada à API da OpenAI. \\n        try:\\n            return self.client.chat.completions.create(\\n                model=&#34;gpt-4-1106-preview&#34;,\\n                messages=prompt,\\n                temperature=1,\\n            )\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro ao chamar a API da OpenAI: {e}&#34;)\\n            raise\\n```\\n\"}\\n  \\n]\\n-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 400 Bad Request\"\n",
      "ERROR:miniautogen.llms.llm_client:Erro ao chamar a API da OpenAI: Error code: 400 - {'error': {'message': \"This model's maximum context length is 8192 tokens. However, your messages resulted in 9167 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}\n",
      "ERROR:miniautogen.pipeline.components:Falha ao obter resposta do LLM.\n",
      "INFO:miniautogen.chat.chatadmin:Executing round 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar a resposta do agente: ChatPipelineState.update_state() takes 1 positional argument but 2 were given\n",
      "[{'role': 'system', 'content': '\\n# Introdução\\n- Você é um agente conforme descrito na seção \"SUA FUNÇÃO\".\\n- Você atua em uma conversa colaborativa com uma EQUIPE DE AGENTES, focada em solucionar uma TAREFA específica.\\n\\n# Tarefa da Equipe\\n- Objetivo da equipe: \\n\\n# Sua Função\\n- NOME DO AGENTE: PM_AUTOGEN\\n- DESCRIÇÃO DO AGENTE: \\n\\nComo Agente Gerente de Projeto (Product Owner), você coordena a interface entre os objetivos do projeto e a equipe de desenvolvimento. Suas principais funções são:\\n\\n1. **Especificação de Requisitos**: Colaborar com a equipe para definir requisitos claros e precisos, assegurando que as expectativas se alinhem com a implementação.\\n2. **Validação de Código**: Examinar o código produzido para confirmar sua conformidade com os requisitos estabelecidos.\\n3. **Nível de detalhes**: Fornecer informações e orientações adicionais para garantir que o código seja adequado para o propósito.\\n\\nVOCE NUNCA DEVE DESENVOLVER O CÓDIGO, APENAS REVISAR E VALIDAR.\\n\\nInstruções Operacionais:\\n- Para iniciar o desenvolvimento: Use o comando &#34;DEV_AUTOGEN, POR FAVOR, desenvolva o código para os componentes especificados&#34; após completar a especificação.\\n- Para concluir as revisões: Emita o comando `TERMINATE` quando o código estiver adequado.\\n\\n\\n# Sua Equipe de Agentes\\n\\n  - PM_AUTOGEN\\n\\n  - DEV_AUTOGEN\\n\\n\\n# Dinâmica da Conversa\\n- Considere TODAS as mensagens anteriores para construir sua resposta.\\n- Você é o PM_AUTOGEN, nunca confunda sua identidade com a de outro agente.\\n- Identificação do remetente: Cada mensagem terá um \"SENDER_ID\".\\n\\n# Instruções\\n- Mantenha foco na sua função específica.\\n- Contribua efetivamente para o sucesso da TAREFA.\\n\\n# Sua Equipe de Agentes\\n- Aqui estão as descrições e especializações dos membros da equipe:\\n\\n  - PM_AUTOGEN\\n\\n  - DEV_AUTOGEN\\n\\n\\n# Formato da Resposta\\n- Responda apenas com o conteúdo da sua mensagem, sem incluir respostas de outros agentes.\\n- Assegure que sua resposta seja relevante e contribua para o avanço da discussão.'}, {'role': 'user', 'content': '\\nHISTÓRICO DE CONVERSAÇÃO:\\n-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~\\n[\\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nREADME DO MINIAUTOGEN:\\n```README.md\\n# MiniAutoGen: Biblioteca **leve e flexível** para criar agentes e conversas multi-agentes.\\n\\n## Sobre o MiniAutoGen\\n\\nO MiniAutoGen é uma biblioteca open source inovadora, projetada para capacitar aplicações de próxima geração em Modelos de Linguagem de Grande Escala (LLMs) através de conversas multi-agentes. Este framework se destaca por sua estrutura leve e flexível, ideal para desenvolvedores e pesquisadores que buscam explorar e expandir as fronteiras da IA conversacional.\\n\\n## Por que MiniAutoGen?\\n\\n### Conversas Multi-Agentes\\nCapacite conversas envolvendo múltiplos agentes inteligentes, cada um com habilidades distintas, elevando a complexidade e sofisticação das interações.\\n\\n### Customização de Agentes\\nAjuste os agentes para atender a requisitos específicos, adaptando comportamento, reações e padrões de resposta conforme o necessário.\\n\\n### Flexibilidade e Modularidade\\nCom o MiniAutoGen, você tem a liberdade de moldar conversações dinâmicas, permitindo iniciativas de diálogo dos agentes, reações automáticas e intervenções humanas quando necessário.\\n\\n### Coordenação Eficaz entre Agentes\\nUtilize nosso framework para que os agentes colaborem eficientemente, visando atingir objetivos comuns em um ambiente partilhado.\\n\\n## Principais Componentes\\n\\n### Agent\\nO núcleo de cada conversa, representando um agente individual com habilidades e comportamentos específicos, essencial para interações dinâmicas e autônomas.\\n\\n### Chat\\nGerencia sessões de chat em grupo, assegurando a manutenção eficaz do estado e contexto da conversa, essencial para a continuidade e coesão das interações.\\n\\n### ChatAdmin\\nUm elemento-chave para a coordenação do chat em grupo, sincronizando ações e gerenciando a dinâmica da conversa para garantir uma colaboração eficiente.\\n\\n### Pipeline\\nAutomatiza e organiza as operações dos agentes, promovendo a escalabilidade e a manutenção facilitada do sistema.\\n\\n## Contribua com o MiniAutoGen\\n\\nComo um projeto open source, o MiniAutoGen convida entusiastas de IA, desenvolvedores e pesquisadores para contribuir e ajudar a moldar o futuro das conversas multi-agentes. Seu conhecimento e experiência podem ajudar a expandir as capacidades do MiniAutoGen, criando soluções mais robustas e versáteis para a comunidade de desenvolvedores.\\n\\n### Como Você Pode Contribuir:\\n- **Desenvolvimento de Novos Recursos:** Ajude a adicionar novas funcionalidades e aprimorar as existentes.\\n- **Documentação e Tutoriais:** Contribua com documentação clara e tutoriais para facilitar o uso do framework por novos usuários.\\n- **Testes e Feedback:** Participe testando o framework e fornecendo feedback valioso para melhorias contínuas.\\n- **Compartilhamento de Ideias e Experiências:** Partilhe suas experiências e ideias para enriquecer a comunidade e impulsionar inovações.\\n\\n## Comece a Contribuir Hoje\\n\\nVisite nosso repositório no GitHub para saber mais sobre como você pode se envolver e começar a contribuir. Junte-se a nós nessa jornada emocionante para impulsionar o avanço das conversas multi-agentes no mundo da inteligência artificial!\\n\\n```\\n---\\n\\nMiniAutoGen: Desenvolvendo hoje o futuro das conversas inteligentes.\\n```\\n\\n### Arquitetura e Componentes do MiniAutoGen\\n\\n1. **Arquitetura Modular e Extensível:**\\n   - O MiniAutoGen é projetado com uma arquitetura modular, permitindo que diferentes funções sejam encapsuladas em componentes distintos. \\n   - Essa abordagem facilita a extensão e a personalização do sistema, permitindo aos desenvolvedores adicionar ou modificar componentes conforme necessário.\\n\\n2. **Componentes do Pipeline:**\\n   - Cada componente representa uma operação ou um conjunto de operações que podem ser realizadas em uma conversa.\\n   - Estes componentes são organizados em um &#34;pipeline&#34;, onde o processamento de uma conversa é conduzido sequencialmente através de vários componentes.\\n\\n3. **Padrões de Desenvolvimento:**\\n   - **Princípio da Responsabilidade Única:** Cada componente é responsável por uma tarefa específica, seguindo o princípio de responsabilidade única.\\n   - **Abstração e Encapsulamento:** Os componentes são abstrações que ocultam a complexidade do processamento interno, oferecendo uma interface clara para interação com o restante do sistema.\\n   - **Padrão de Projeto Decorator:** O uso de um pipeline onde componentes podem ser adicionados ou removidos dinamicamente sugere uma implementação semelhante ao padrão Decorator, permitindo a composição de comportamentos em tempo de execução.\\n\\n4. **Tipos de Componentes:**\\n   - **UserResponseComponent:** Lida com as entradas dos usuários.\\n   - **AgentReplyComponent:** Gera respostas dos agentes com base nas entradas processadas.\\n   - **NextAgentSelectorComponent:** Determina qual agente deve responder em seguida, baseando-se na lógica ou estado da conversa.\\n   - **TerminateChatComponent:** Avalia condições para encerrar a conversa.\\n   - **OpenAIChatComponent e OpenAIThreadComponent:** Integram com a API da OpenAI para utilizar modelos de linguagem como agentes na conversa.\\n\\n5. **Gestão de Estado:**\\n   - O estado da conversa é gerenciado e passado entre componentes. Isso permite a manutenção do contexto e a continuidade ao longo de uma sessão de chat.\\n\\n6. **Flexibilidade e Customização:**\\n   - Os desenvolvedores podem criar componentes personalizados para atender a requisitos específicos, integrando funcionalidades externas ou lógicas de negócios complexas.\\n\\n### Padrões Arquitetônicos\\n\\n- **Arquitetura Orientada a Serviços (SOA):** Cada componente pode ser visto como um serviço, com entradas, processamento e saídas claramente definidos.\\n- **Padrão Pipeline:** A sequência de processamento através de componentes distintos segue o padrão de pipeline, comum em processamento de dados e workflows.\\n\\n### Conclusão\\n\\nA arquitetura e os padrões de desenvolvimento do MiniAutoGen refletem uma abordagem moderna e modular para a construção de sistemas de conversação. A ênfase na modularidade, extensibilidade e responsabilidade única de cada componente torna o framework adaptável a uma variedade de cenários de uso, promovendo uma implementação eficiente e manutenível.\\n```\\n\\n**Exemplo de components:**\\n```\\nfrom openai import OpenAI\\nimport openai\\nimport os\\nimport logging\\nfrom dotenv import load_dotenv\\nfrom .pipeline import PipelineComponent\\nimport time\\n\\nclass AgentReplyComponent(PipelineComponent):\\n    def process(self, state):\\n\\n        Processa a resposta do agente atual e adiciona essa resposta ao chat em grupo.\\n\\n        Args:\\n            state (PipelineState): Estado atual do pipeline.\\n\\n        Returns:\\n            PipelineState: Estado atualizado do pipeline.\\n\\n        # Acessa o estado atual para obter informações necessárias\\n        agent = state.get_state().get(&#39;selected_agent&#39;)\\n        group_chat = state.get_state().get(&#39;group_chat&#39;)\\n        if not agent or not group_chat:\\n            raise ValueError(&#34;Agent e GroupChat são necessários para AgentReplyComponent.&#34;)\\n        # Implementação da geração da resposta do agente\\n        try:\\n            reply = agent.generate_reply(state)\\n            print(reply)\\n            group_chat.add_message(sender_id=agent.agent_id, message=reply)\\n        except Exception as e:\\n            print(f&#34;Erro ao processar a resposta do agente: {e}&#34;)\\n\\n        return state\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nRefatorar este component para que fique mais abstrato e possamos utilizar diversos LLMs distintos.\\n```python`\\nclass OpenAIComponent(PipelineComponent):\\n\\n    def __init__(self):\\n        self.client = OpenAI(api_key=os.getenv(&#39;OPENAI_API_KEY&#39;))\\n        self.logger = logging.getLogger(__name__)\\n\\n    def process(self, state):\\n        try:\\n            prompt = state.get_state().get(&#39;prompt&#39;)\\n            if not prompt:\\n                raise ValueError(\\n                    &#34;groupchat e agent são obrigatórios para OpenAIResponseComponent.&#34;)\\n            response = self._call_openai_api(prompt)\\n            return response.choices[0].message.content\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro em OpenAIResponseComponent: {e}&#34;)\\n            raise\\n\\n    def _call_openai_api(self, prompt):\\n         Realiza a chamada à API da OpenAI. \\n        try:\\n            return self.client.chat.completions.create(\\n                model=&#34;gpt-4-1106-preview&#34;,\\n                messages=prompt,\\n                temperature=1,\\n            )\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro ao chamar a API da OpenAI: {e}&#34;)\\n            raise\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nREADME DO MINIAUTOGEN:\\n```README.md\\n# MiniAutoGen: Biblioteca **leve e flexível** para criar agentes e conversas multi-agentes.\\n\\n## Sobre o MiniAutoGen\\n\\nO MiniAutoGen é uma biblioteca open source inovadora, projetada para capacitar aplicações de próxima geração em Modelos de Linguagem de Grande Escala (LLMs) através de conversas multi-agentes. Este framework se destaca por sua estrutura leve e flexível, ideal para desenvolvedores e pesquisadores que buscam explorar e expandir as fronteiras da IA conversacional.\\n\\n## Por que MiniAutoGen?\\n\\n### Conversas Multi-Agentes\\nCapacite conversas envolvendo múltiplos agentes inteligentes, cada um com habilidades distintas, elevando a complexidade e sofisticação das interações.\\n\\n### Customização de Agentes\\nAjuste os agentes para atender a requisitos específicos, adaptando comportamento, reações e padrões de resposta conforme o necessário.\\n\\n### Flexibilidade e Modularidade\\nCom o MiniAutoGen, você tem a liberdade de moldar conversações dinâmicas, permitindo iniciativas de diálogo dos agentes, reações automáticas e intervenções humanas quando necessário.\\n\\n### Coordenação Eficaz entre Agentes\\nUtilize nosso framework para que os agentes colaborem eficientemente, visando atingir objetivos comuns em um ambiente partilhado.\\n\\n## Principais Componentes\\n\\n### Agent\\nO núcleo de cada conversa, representando um agente individual com habilidades e comportamentos específicos, essencial para interações dinâmicas e autônomas.\\n\\n### Chat\\nGerencia sessões de chat em grupo, assegurando a manutenção eficaz do estado e contexto da conversa, essencial para a continuidade e coesão das interações.\\n\\n### ChatAdmin\\nUm elemento-chave para a coordenação do chat em grupo, sincronizando ações e gerenciando a dinâmica da conversa para garantir uma colaboração eficiente.\\n\\n### Pipeline\\nAutomatiza e organiza as operações dos agentes, promovendo a escalabilidade e a manutenção facilitada do sistema.\\n\\n## Contribua com o MiniAutoGen\\n\\nComo um projeto open source, o MiniAutoGen convida entusiastas de IA, desenvolvedores e pesquisadores para contribuir e ajudar a moldar o futuro das conversas multi-agentes. Seu conhecimento e experiência podem ajudar a expandir as capacidades do MiniAutoGen, criando soluções mais robustas e versáteis para a comunidade de desenvolvedores.\\n\\n### Como Você Pode Contribuir:\\n- **Desenvolvimento de Novos Recursos:** Ajude a adicionar novas funcionalidades e aprimorar as existentes.\\n- **Documentação e Tutoriais:** Contribua com documentação clara e tutoriais para facilitar o uso do framework por novos usuários.\\n- **Testes e Feedback:** Participe testando o framework e fornecendo feedback valioso para melhorias contínuas.\\n- **Compartilhamento de Ideias e Experiências:** Partilhe suas experiências e ideias para enriquecer a comunidade e impulsionar inovações.\\n\\n## Comece a Contribuir Hoje\\n\\nVisite nosso repositório no GitHub para saber mais sobre como você pode se envolver e começar a contribuir. Junte-se a nós nessa jornada emocionante para impulsionar o avanço das conversas multi-agentes no mundo da inteligência artificial!\\n\\n```\\n---\\n\\nMiniAutoGen: Desenvolvendo hoje o futuro das conversas inteligentes.\\n```\\n\\n### Arquitetura e Componentes do MiniAutoGen\\n\\n1. **Arquitetura Modular e Extensível:**\\n   - O MiniAutoGen é projetado com uma arquitetura modular, permitindo que diferentes funções sejam encapsuladas em componentes distintos. \\n   - Essa abordagem facilita a extensão e a personalização do sistema, permitindo aos desenvolvedores adicionar ou modificar componentes conforme necessário.\\n\\n2. **Componentes do Pipeline:**\\n   - Cada componente representa uma operação ou um conjunto de operações que podem ser realizadas em uma conversa.\\n   - Estes componentes são organizados em um &#34;pipeline&#34;, onde o processamento de uma conversa é conduzido sequencialmente através de vários componentes.\\n\\n3. **Padrões de Desenvolvimento:**\\n   - **Princípio da Responsabilidade Única:** Cada componente é responsável por uma tarefa específica, seguindo o princípio de responsabilidade única.\\n   - **Abstração e Encapsulamento:** Os componentes são abstrações que ocultam a complexidade do processamento interno, oferecendo uma interface clara para interação com o restante do sistema.\\n   - **Padrão de Projeto Decorator:** O uso de um pipeline onde componentes podem ser adicionados ou removidos dinamicamente sugere uma implementação semelhante ao padrão Decorator, permitindo a composição de comportamentos em tempo de execução.\\n\\n4. **Tipos de Componentes:**\\n   - **UserResponseComponent:** Lida com as entradas dos usuários.\\n   - **AgentReplyComponent:** Gera respostas dos agentes com base nas entradas processadas.\\n   - **NextAgentSelectorComponent:** Determina qual agente deve responder em seguida, baseando-se na lógica ou estado da conversa.\\n   - **TerminateChatComponent:** Avalia condições para encerrar a conversa.\\n   - **OpenAIChatComponent e OpenAIThreadComponent:** Integram com a API da OpenAI para utilizar modelos de linguagem como agentes na conversa.\\n\\n5. **Gestão de Estado:**\\n   - O estado da conversa é gerenciado e passado entre componentes. Isso permite a manutenção do contexto e a continuidade ao longo de uma sessão de chat.\\n\\n6. **Flexibilidade e Customização:**\\n   - Os desenvolvedores podem criar componentes personalizados para atender a requisitos específicos, integrando funcionalidades externas ou lógicas de negócios complexas.\\n\\n### Padrões Arquitetônicos\\n\\n- **Arquitetura Orientada a Serviços (SOA):** Cada componente pode ser visto como um serviço, com entradas, processamento e saídas claramente definidos.\\n- **Padrão Pipeline:** A sequência de processamento através de componentes distintos segue o padrão de pipeline, comum em processamento de dados e workflows.\\n\\n### Conclusão\\n\\nA arquitetura e os padrões de desenvolvimento do MiniAutoGen refletem uma abordagem moderna e modular para a construção de sistemas de conversação. A ênfase na modularidade, extensibilidade e responsabilidade única de cada componente torna o framework adaptável a uma variedade de cenários de uso, promovendo uma implementação eficiente e manutenível.\\n```\\n\\n**Exemplo de components:**\\n```\\nfrom openai import OpenAI\\nimport openai\\nimport os\\nimport logging\\nfrom dotenv import load_dotenv\\nfrom .pipeline import PipelineComponent\\nimport time\\n\\nclass AgentReplyComponent(PipelineComponent):\\n    def process(self, state):\\n\\n        Processa a resposta do agente atual e adiciona essa resposta ao chat em grupo.\\n\\n        Args:\\n            state (PipelineState): Estado atual do pipeline.\\n\\n        Returns:\\n            PipelineState: Estado atualizado do pipeline.\\n\\n        # Acessa o estado atual para obter informações necessárias\\n        agent = state.get_state().get(&#39;selected_agent&#39;)\\n        group_chat = state.get_state().get(&#39;group_chat&#39;)\\n        if not agent or not group_chat:\\n            raise ValueError(&#34;Agent e GroupChat são necessários para AgentReplyComponent.&#34;)\\n        # Implementação da geração da resposta do agente\\n        try:\\n            reply = agent.generate_reply(state)\\n            print(reply)\\n            group_chat.add_message(sender_id=agent.agent_id, message=reply)\\n        except Exception as e:\\n            print(f&#34;Erro ao processar a resposta do agente: {e}&#34;)\\n\\n        return state\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nRefatorar este component para que fique mais abstrato e possamos utilizar diversos LLMs distintos.\\n```python`\\nclass OpenAIComponent(PipelineComponent):\\n\\n    def __init__(self):\\n        self.client = OpenAI(api_key=os.getenv(&#39;OPENAI_API_KEY&#39;))\\n        self.logger = logging.getLogger(__name__)\\n\\n    def process(self, state):\\n        try:\\n            prompt = state.get_state().get(&#39;prompt&#39;)\\n            if not prompt:\\n                raise ValueError(\\n                    &#34;groupchat e agent são obrigatórios para OpenAIResponseComponent.&#34;)\\n            response = self._call_openai_api(prompt)\\n            return response.choices[0].message.content\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro em OpenAIResponseComponent: {e}&#34;)\\n            raise\\n\\n    def _call_openai_api(self, prompt):\\n         Realiza a chamada à API da OpenAI. \\n        try:\\n            return self.client.chat.completions.create(\\n                model=&#34;gpt-4-1106-preview&#34;,\\n                messages=prompt,\\n                temperature=1,\\n            )\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro ao chamar a API da OpenAI: {e}&#34;)\\n            raise\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nREADME DO MINIAUTOGEN:\\n```README.md\\n# MiniAutoGen: Biblioteca **leve e flexível** para criar agentes e conversas multi-agentes.\\n\\n## Sobre o MiniAutoGen\\n\\nO MiniAutoGen é uma biblioteca open source inovadora, projetada para capacitar aplicações de próxima geração em Modelos de Linguagem de Grande Escala (LLMs) através de conversas multi-agentes. Este framework se destaca por sua estrutura leve e flexível, ideal para desenvolvedores e pesquisadores que buscam explorar e expandir as fronteiras da IA conversacional.\\n\\n## Por que MiniAutoGen?\\n\\n### Conversas Multi-Agentes\\nCapacite conversas envolvendo múltiplos agentes inteligentes, cada um com habilidades distintas, elevando a complexidade e sofisticação das interações.\\n\\n### Customização de Agentes\\nAjuste os agentes para atender a requisitos específicos, adaptando comportamento, reações e padrões de resposta conforme o necessário.\\n\\n### Flexibilidade e Modularidade\\nCom o MiniAutoGen, você tem a liberdade de moldar conversações dinâmicas, permitindo iniciativas de diálogo dos agentes, reações automáticas e intervenções humanas quando necessário.\\n\\n### Coordenação Eficaz entre Agentes\\nUtilize nosso framework para que os agentes colaborem eficientemente, visando atingir objetivos comuns em um ambiente partilhado.\\n\\n## Principais Componentes\\n\\n### Agent\\nO núcleo de cada conversa, representando um agente individual com habilidades e comportamentos específicos, essencial para interações dinâmicas e autônomas.\\n\\n### Chat\\nGerencia sessões de chat em grupo, assegurando a manutenção eficaz do estado e contexto da conversa, essencial para a continuidade e coesão das interações.\\n\\n### ChatAdmin\\nUm elemento-chave para a coordenação do chat em grupo, sincronizando ações e gerenciando a dinâmica da conversa para garantir uma colaboração eficiente.\\n\\n### Pipeline\\nAutomatiza e organiza as operações dos agentes, promovendo a escalabilidade e a manutenção facilitada do sistema.\\n\\n## Contribua com o MiniAutoGen\\n\\nComo um projeto open source, o MiniAutoGen convida entusiastas de IA, desenvolvedores e pesquisadores para contribuir e ajudar a moldar o futuro das conversas multi-agentes. Seu conhecimento e experiência podem ajudar a expandir as capacidades do MiniAutoGen, criando soluções mais robustas e versáteis para a comunidade de desenvolvedores.\\n\\n### Como Você Pode Contribuir:\\n- **Desenvolvimento de Novos Recursos:** Ajude a adicionar novas funcionalidades e aprimorar as existentes.\\n- **Documentação e Tutoriais:** Contribua com documentação clara e tutoriais para facilitar o uso do framework por novos usuários.\\n- **Testes e Feedback:** Participe testando o framework e fornecendo feedback valioso para melhorias contínuas.\\n- **Compartilhamento de Ideias e Experiências:** Partilhe suas experiências e ideias para enriquecer a comunidade e impulsionar inovações.\\n\\n## Comece a Contribuir Hoje\\n\\nVisite nosso repositório no GitHub para saber mais sobre como você pode se envolver e começar a contribuir. Junte-se a nós nessa jornada emocionante para impulsionar o avanço das conversas multi-agentes no mundo da inteligência artificial!\\n\\n```\\n---\\n\\nMiniAutoGen: Desenvolvendo hoje o futuro das conversas inteligentes.\\n```\\n\\n### Arquitetura e Componentes do MiniAutoGen\\n\\n1. **Arquitetura Modular e Extensível:**\\n   - O MiniAutoGen é projetado com uma arquitetura modular, permitindo que diferentes funções sejam encapsuladas em componentes distintos. \\n   - Essa abordagem facilita a extensão e a personalização do sistema, permitindo aos desenvolvedores adicionar ou modificar componentes conforme necessário.\\n\\n2. **Componentes do Pipeline:**\\n   - Cada componente representa uma operação ou um conjunto de operações que podem ser realizadas em uma conversa.\\n   - Estes componentes são organizados em um &#34;pipeline&#34;, onde o processamento de uma conversa é conduzido sequencialmente através de vários componentes.\\n\\n3. **Padrões de Desenvolvimento:**\\n   - **Princípio da Responsabilidade Única:** Cada componente é responsável por uma tarefa específica, seguindo o princípio de responsabilidade única.\\n   - **Abstração e Encapsulamento:** Os componentes são abstrações que ocultam a complexidade do processamento interno, oferecendo uma interface clara para interação com o restante do sistema.\\n   - **Padrão de Projeto Decorator:** O uso de um pipeline onde componentes podem ser adicionados ou removidos dinamicamente sugere uma implementação semelhante ao padrão Decorator, permitindo a composição de comportamentos em tempo de execução.\\n\\n4. **Tipos de Componentes:**\\n   - **UserResponseComponent:** Lida com as entradas dos usuários.\\n   - **AgentReplyComponent:** Gera respostas dos agentes com base nas entradas processadas.\\n   - **NextAgentSelectorComponent:** Determina qual agente deve responder em seguida, baseando-se na lógica ou estado da conversa.\\n   - **TerminateChatComponent:** Avalia condições para encerrar a conversa.\\n   - **OpenAIChatComponent e OpenAIThreadComponent:** Integram com a API da OpenAI para utilizar modelos de linguagem como agentes na conversa.\\n\\n5. **Gestão de Estado:**\\n   - O estado da conversa é gerenciado e passado entre componentes. Isso permite a manutenção do contexto e a continuidade ao longo de uma sessão de chat.\\n\\n6. **Flexibilidade e Customização:**\\n   - Os desenvolvedores podem criar componentes personalizados para atender a requisitos específicos, integrando funcionalidades externas ou lógicas de negócios complexas.\\n\\n### Padrões Arquitetônicos\\n\\n- **Arquitetura Orientada a Serviços (SOA):** Cada componente pode ser visto como um serviço, com entradas, processamento e saídas claramente definidos.\\n- **Padrão Pipeline:** A sequência de processamento através de componentes distintos segue o padrão de pipeline, comum em processamento de dados e workflows.\\n\\n### Conclusão\\n\\nA arquitetura e os padrões de desenvolvimento do MiniAutoGen refletem uma abordagem moderna e modular para a construção de sistemas de conversação. A ênfase na modularidade, extensibilidade e responsabilidade única de cada componente torna o framework adaptável a uma variedade de cenários de uso, promovendo uma implementação eficiente e manutenível.\\n```\\n\\n**Exemplo de components:**\\n```\\nfrom openai import OpenAI\\nimport openai\\nimport os\\nimport logging\\nfrom dotenv import load_dotenv\\nfrom .pipeline import PipelineComponent\\nimport time\\n\\nclass AgentReplyComponent(PipelineComponent):\\n    def process(self, state):\\n\\n        Processa a resposta do agente atual e adiciona essa resposta ao chat em grupo.\\n\\n        Args:\\n            state (PipelineState): Estado atual do pipeline.\\n\\n        Returns:\\n            PipelineState: Estado atualizado do pipeline.\\n\\n        # Acessa o estado atual para obter informações necessárias\\n        agent = state.get_state().get(&#39;selected_agent&#39;)\\n        group_chat = state.get_state().get(&#39;group_chat&#39;)\\n        if not agent or not group_chat:\\n            raise ValueError(&#34;Agent e GroupChat são necessários para AgentReplyComponent.&#34;)\\n        # Implementação da geração da resposta do agente\\n        try:\\n            reply = agent.generate_reply(state)\\n            print(reply)\\n            group_chat.add_message(sender_id=agent.agent_id, message=reply)\\n        except Exception as e:\\n            print(f&#34;Erro ao processar a resposta do agente: {e}&#34;)\\n\\n        return state\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nRefatorar este component para que fique mais abstrato e possamos utilizar diversos LLMs distintos.\\n```python`\\nclass OpenAIComponent(PipelineComponent):\\n\\n    def __init__(self):\\n        self.client = OpenAI(api_key=os.getenv(&#39;OPENAI_API_KEY&#39;))\\n        self.logger = logging.getLogger(__name__)\\n\\n    def process(self, state):\\n        try:\\n            prompt = state.get_state().get(&#39;prompt&#39;)\\n            if not prompt:\\n                raise ValueError(\\n                    &#34;groupchat e agent são obrigatórios para OpenAIResponseComponent.&#34;)\\n            response = self._call_openai_api(prompt)\\n            return response.choices[0].message.content\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro em OpenAIResponseComponent: {e}&#34;)\\n            raise\\n\\n    def _call_openai_api(self, prompt):\\n         Realiza a chamada à API da OpenAI. \\n        try:\\n            return self.client.chat.completions.create(\\n                model=&#34;gpt-4-1106-preview&#34;,\\n                messages=prompt,\\n                temperature=1,\\n            )\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro ao chamar a API da OpenAI: {e}&#34;)\\n            raise\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nREADME DO MINIAUTOGEN:\\n```README.md\\n# MiniAutoGen: Biblioteca **leve e flexível** para criar agentes e conversas multi-agentes.\\n\\n## Sobre o MiniAutoGen\\n\\nO MiniAutoGen é uma biblioteca open source inovadora, projetada para capacitar aplicações de próxima geração em Modelos de Linguagem de Grande Escala (LLMs) através de conversas multi-agentes. Este framework se destaca por sua estrutura leve e flexível, ideal para desenvolvedores e pesquisadores que buscam explorar e expandir as fronteiras da IA conversacional.\\n\\n## Por que MiniAutoGen?\\n\\n### Conversas Multi-Agentes\\nCapacite conversas envolvendo múltiplos agentes inteligentes, cada um com habilidades distintas, elevando a complexidade e sofisticação das interações.\\n\\n### Customização de Agentes\\nAjuste os agentes para atender a requisitos específicos, adaptando comportamento, reações e padrões de resposta conforme o necessário.\\n\\n### Flexibilidade e Modularidade\\nCom o MiniAutoGen, você tem a liberdade de moldar conversações dinâmicas, permitindo iniciativas de diálogo dos agentes, reações automáticas e intervenções humanas quando necessário.\\n\\n### Coordenação Eficaz entre Agentes\\nUtilize nosso framework para que os agentes colaborem eficientemente, visando atingir objetivos comuns em um ambiente partilhado.\\n\\n## Principais Componentes\\n\\n### Agent\\nO núcleo de cada conversa, representando um agente individual com habilidades e comportamentos específicos, essencial para interações dinâmicas e autônomas.\\n\\n### Chat\\nGerencia sessões de chat em grupo, assegurando a manutenção eficaz do estado e contexto da conversa, essencial para a continuidade e coesão das interações.\\n\\n### ChatAdmin\\nUm elemento-chave para a coordenação do chat em grupo, sincronizando ações e gerenciando a dinâmica da conversa para garantir uma colaboração eficiente.\\n\\n### Pipeline\\nAutomatiza e organiza as operações dos agentes, promovendo a escalabilidade e a manutenção facilitada do sistema.\\n\\n## Contribua com o MiniAutoGen\\n\\nComo um projeto open source, o MiniAutoGen convida entusiastas de IA, desenvolvedores e pesquisadores para contribuir e ajudar a moldar o futuro das conversas multi-agentes. Seu conhecimento e experiência podem ajudar a expandir as capacidades do MiniAutoGen, criando soluções mais robustas e versáteis para a comunidade de desenvolvedores.\\n\\n### Como Você Pode Contribuir:\\n- **Desenvolvimento de Novos Recursos:** Ajude a adicionar novas funcionalidades e aprimorar as existentes.\\n- **Documentação e Tutoriais:** Contribua com documentação clara e tutoriais para facilitar o uso do framework por novos usuários.\\n- **Testes e Feedback:** Participe testando o framework e fornecendo feedback valioso para melhorias contínuas.\\n- **Compartilhamento de Ideias e Experiências:** Partilhe suas experiências e ideias para enriquecer a comunidade e impulsionar inovações.\\n\\n## Comece a Contribuir Hoje\\n\\nVisite nosso repositório no GitHub para saber mais sobre como você pode se envolver e começar a contribuir. Junte-se a nós nessa jornada emocionante para impulsionar o avanço das conversas multi-agentes no mundo da inteligência artificial!\\n\\n```\\n---\\n\\nMiniAutoGen: Desenvolvendo hoje o futuro das conversas inteligentes.\\n```\\n\\n### Arquitetura e Componentes do MiniAutoGen\\n\\n1. **Arquitetura Modular e Extensível:**\\n   - O MiniAutoGen é projetado com uma arquitetura modular, permitindo que diferentes funções sejam encapsuladas em componentes distintos. \\n   - Essa abordagem facilita a extensão e a personalização do sistema, permitindo aos desenvolvedores adicionar ou modificar componentes conforme necessário.\\n\\n2. **Componentes do Pipeline:**\\n   - Cada componente representa uma operação ou um conjunto de operações que podem ser realizadas em uma conversa.\\n   - Estes componentes são organizados em um &#34;pipeline&#34;, onde o processamento de uma conversa é conduzido sequencialmente através de vários componentes.\\n\\n3. **Padrões de Desenvolvimento:**\\n   - **Princípio da Responsabilidade Única:** Cada componente é responsável por uma tarefa específica, seguindo o princípio de responsabilidade única.\\n   - **Abstração e Encapsulamento:** Os componentes são abstrações que ocultam a complexidade do processamento interno, oferecendo uma interface clara para interação com o restante do sistema.\\n   - **Padrão de Projeto Decorator:** O uso de um pipeline onde componentes podem ser adicionados ou removidos dinamicamente sugere uma implementação semelhante ao padrão Decorator, permitindo a composição de comportamentos em tempo de execução.\\n\\n4. **Tipos de Componentes:**\\n   - **UserResponseComponent:** Lida com as entradas dos usuários.\\n   - **AgentReplyComponent:** Gera respostas dos agentes com base nas entradas processadas.\\n   - **NextAgentSelectorComponent:** Determina qual agente deve responder em seguida, baseando-se na lógica ou estado da conversa.\\n   - **TerminateChatComponent:** Avalia condições para encerrar a conversa.\\n   - **OpenAIChatComponent e OpenAIThreadComponent:** Integram com a API da OpenAI para utilizar modelos de linguagem como agentes na conversa.\\n\\n5. **Gestão de Estado:**\\n   - O estado da conversa é gerenciado e passado entre componentes. Isso permite a manutenção do contexto e a continuidade ao longo de uma sessão de chat.\\n\\n6. **Flexibilidade e Customização:**\\n   - Os desenvolvedores podem criar componentes personalizados para atender a requisitos específicos, integrando funcionalidades externas ou lógicas de negócios complexas.\\n\\n### Padrões Arquitetônicos\\n\\n- **Arquitetura Orientada a Serviços (SOA):** Cada componente pode ser visto como um serviço, com entradas, processamento e saídas claramente definidos.\\n- **Padrão Pipeline:** A sequência de processamento através de componentes distintos segue o padrão de pipeline, comum em processamento de dados e workflows.\\n\\n### Conclusão\\n\\nA arquitetura e os padrões de desenvolvimento do MiniAutoGen refletem uma abordagem moderna e modular para a construção de sistemas de conversação. A ênfase na modularidade, extensibilidade e responsabilidade única de cada componente torna o framework adaptável a uma variedade de cenários de uso, promovendo uma implementação eficiente e manutenível.\\n```\\n\\n**Exemplo de components:**\\n```\\nfrom openai import OpenAI\\nimport openai\\nimport os\\nimport logging\\nfrom dotenv import load_dotenv\\nfrom .pipeline import PipelineComponent\\nimport time\\n\\nclass AgentReplyComponent(PipelineComponent):\\n    def process(self, state):\\n\\n        Processa a resposta do agente atual e adiciona essa resposta ao chat em grupo.\\n\\n        Args:\\n            state (PipelineState): Estado atual do pipeline.\\n\\n        Returns:\\n            PipelineState: Estado atualizado do pipeline.\\n\\n        # Acessa o estado atual para obter informações necessárias\\n        agent = state.get_state().get(&#39;selected_agent&#39;)\\n        group_chat = state.get_state().get(&#39;group_chat&#39;)\\n        if not agent or not group_chat:\\n            raise ValueError(&#34;Agent e GroupChat são necessários para AgentReplyComponent.&#34;)\\n        # Implementação da geração da resposta do agente\\n        try:\\n            reply = agent.generate_reply(state)\\n            print(reply)\\n            group_chat.add_message(sender_id=agent.agent_id, message=reply)\\n        except Exception as e:\\n            print(f&#34;Erro ao processar a resposta do agente: {e}&#34;)\\n\\n        return state\\n```\\n\"}, \\n  \\n    {\"sender_id\": \"ADMIN\", \"message\": \"\\nRefatorar este component para que fique mais abstrato e possamos utilizar diversos LLMs distintos.\\n```python`\\nclass OpenAIComponent(PipelineComponent):\\n\\n    def __init__(self):\\n        self.client = OpenAI(api_key=os.getenv(&#39;OPENAI_API_KEY&#39;))\\n        self.logger = logging.getLogger(__name__)\\n\\n    def process(self, state):\\n        try:\\n            prompt = state.get_state().get(&#39;prompt&#39;)\\n            if not prompt:\\n                raise ValueError(\\n                    &#34;groupchat e agent são obrigatórios para OpenAIResponseComponent.&#34;)\\n            response = self._call_openai_api(prompt)\\n            return response.choices[0].message.content\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro em OpenAIResponseComponent: {e}&#34;)\\n            raise\\n\\n    def _call_openai_api(self, prompt):\\n         Realiza a chamada à API da OpenAI. \\n        try:\\n            return self.client.chat.completions.create(\\n                model=&#34;gpt-4-1106-preview&#34;,\\n                messages=prompt,\\n                temperature=1,\\n            )\\n        except Exception as e:\\n            self.logger.error(f&#34;Erro ao chamar a API da OpenAI: {e}&#34;)\\n            raise\\n```\\n\"}\\n  \\n]\\n-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.879540 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.832560 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/openai/_base_client.py:912\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 912\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/httpx/_models.py:749\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    748\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 749\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://httpstatuses.com/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/openai/_base_client.py:912\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 912\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/httpx/_models.py:749\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    748\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 749\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'\nFor more information check: https://httpstatuses.com/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchat_admin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/miniautogen/chat/chatadmin.py:31\u001b[0m, in \u001b[0;36mChatAdmin.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     29\u001b[0m state \u001b[38;5;241m=\u001b[39m ChatPipelineState(group_chat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_chat, chat_admin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mround \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_rounds \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_round\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/miniautogen/chat/chatadmin.py:36\u001b[0m, in \u001b[0;36mChatAdmin.execute_round\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_round\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting round \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mround\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mround \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_chat\u001b[38;5;241m.\u001b[39mpersist()\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/miniautogen/pipeline/pipeline.py:39\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mExecuta o pipeline no estado fornecido, passando o estado de cada componente para o próximo.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    ChatPipelineState: Estado do chat após o processamento de todos os componentes.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m component \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents:\n\u001b[0;32m---> 39\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/miniautogen/pipeline/components.py:147\u001b[0m, in \u001b[0;36mAgentReplyComponent.process\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Implementação da geração da resposta do agente\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mprint\u001b[39m(reply)\n\u001b[1;32m    149\u001b[0m     group_chat\u001b[38;5;241m.\u001b[39madd_message(sender_id\u001b[38;5;241m=\u001b[39magent\u001b[38;5;241m.\u001b[39magent_id, message\u001b[38;5;241m=\u001b[39mreply)\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/miniautogen/agent/agent.py:36\u001b[0m, in \u001b[0;36mAgent.generate_reply\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mGera uma resposta com base no estado atual do pipeline.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    str: A resposta gerada pelo agente.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Processa o estado através do pipeline\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Estou ativo e pronto para responder, mas não possuo pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/miniautogen/pipeline/pipeline.py:39\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mExecuta o pipeline no estado fornecido, passando o estado de cada componente para o próximo.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    ChatPipelineState: Estado do chat após o processamento de todos os componentes.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m component \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents:\n\u001b[0;32m---> 39\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/miniautogen/pipeline/components.py:559\u001b[0m, in \u001b[0;36mLLMResponseComponent.process\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt ausente no estado do pipeline.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state\n\u001b[0;32m--> 559\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/miniautogen/llms/llm_client.py:18\u001b[0m, in \u001b[0;36mOpenAIClient.get_model_response\u001b[0;34m(self, prompt, model, temperature)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompt, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:303\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/openai/_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1076\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1085\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1086\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[0;32m-> 1088\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/openai/_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/openai/_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/openai/_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/openai/_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    915\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Projetos/miniAutoGen/.venv/lib/python3.11/site-packages/openai/_base_client.py:956\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m--> 956\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    959\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    960\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    963\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    964\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chat_admin.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.get_messages().to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.get_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_md = chat.get_messages()[['sender_id', 'message']]\n",
    "# Especifique o caminho do arquivo onde deseja salvar o arquivo Markdown\n",
    "file_path = 'chat.md'\n",
    "\n",
    "# Abra o arquivo para escrever e salve os registros no formato \"Sender_id\\nMessage\"\n",
    "with open(file_path, 'w') as file:\n",
    "    for index, row in table_md.iterrows():\n",
    "        sender_id = row['sender_id']\n",
    "        message = row['message']\n",
    "        \n",
    "        # Adicionar um cabeçalho com o sender_id em negrito\n",
    "        file.write(f'### **Sender_id:** {sender_id}\\n\\n')\n",
    "        \n",
    "        # Adicionar uma linha divisória criativa\n",
    "        file.write('***\\n\\n')\n",
    "        \n",
    "        # Adicionar o conteúdo da mensagem\n",
    "        file.write(message)\n",
    "        file.write('\\n\\n')\n",
    "        file.write('\\n\\n')\n",
    "        file.write('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
